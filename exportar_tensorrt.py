"""
Script para exportar modelo YOLO a TensorRT (optimizado para Jetson Orin)
Ejecutar en la Jetson: python3 exportar_tensorrt.py
"""

from ultralytics import YOLO
import torch

print("=" * 60)
print("EXPORTACI√ìN A TENSORRT PARA JETSON ORIN")
print("=" * 60)

# Verificar GPU
if not torch.cuda.is_available():
    print("‚ùå CUDA no disponible. TensorRT requiere GPU.")
    exit(1)

print(f"\n‚úÖ GPU disponible: {torch.cuda.get_device_name(0)}")
print(f"‚úÖ CUDA versi√≥n: {torch.version.cuda}")

# ============================================
# CONFIGURACI√ìN - AJUSTA ESTOS VALORES
# ============================================
MODELO_ENTRADA = "modelos/yolov8n.pt"  # Modelo PyTorch de origen
MODELO_SALIDA = "modelos/yolov8n.engine"  # Nombre del archivo TensorRT resultante

# Par√°metros de exportaci√≥n (ajusta seg√∫n necesites)
CONFIG = {
    "format": "engine",      # Formato TensorRT
    "device": 0,             # GPU 0 (Orin)
    "imgsz": 480,            # Tama√±o de imagen (debe coincidir con tu c√≥digo)
    "half": True,            # FP16 (m√°s r√°pido que FP32, menos preciso)
    "dynamic": False,        # Entrada fija (m√°s r√°pido que dynamic=True)
    "workspace": 4,          # Memoria de trabajo en GB
    "int8": False,           # INT8 (m√°s r√°pido pero requiere calibraci√≥n)
    # "data": "coco.yaml",   # Dataset para calibraci√≥n INT8 (descomentar si int8=True)
    # "batch": 1,             # Tama√±o de batch (por defecto 1, ajustar si procesas lotes)
}

# ============================================
# EXPORTACI√ìN
# ============================================

print(f"\nüì¶ Cargando modelo: {MODELO_ENTRADA}")
modelo = YOLO(MODELO_ENTRADA)

print("\nüöÄ Exportando a TensorRT...")
print("   Configuraci√≥n:")
for key, value in CONFIG.items():
    print(f"   - {key}: {value}")

print("\n   ‚è≥ Esto puede tomar varios minutos...")
print("   ‚è≥ TensorRT optimizar√° el modelo espec√≠ficamente para tu Jetson Orin")

try:
    # Exportar
    modelo.export(**CONFIG)
    
    print(f"\n‚úÖ Exportaci√≥n completada!")
    print(f"   Archivo generado: {MODELO_SALIDA}")
    print("\nüìù PR√ìXIMOS PASOS:")
    print("   1. El c√≥digo ya est√° configurado para usar TensorRT autom√°ticamente")
    print("   2. Reinicia el servidor: uvicorn main_ind:app --host 0.0.0.0 --port 8000")
    print("   3. Deber√≠as ver: 'üöÄ Cargando modelo TensorRT...'")
    print("\nüí° Para experimentar con otros par√°metros:")
    print("   - Cambia los valores en CONFIG arriba")
    print("   - Ejecuta este script de nuevo")
    print("   - Compara rendimiento")
    
except Exception as e:
    print(f"\n‚ùå Error durante la exportaci√≥n: {e}")
    print("\nPosibles causas:")
    print("   - TensorRT no est√° instalado en JetPack")
    print("   - Falta memoria GPU")
    print("   - Versi√≥n incompatible")
    print("\nSoluci√≥n alternativa: Usar PyTorch optimizado (ya configurado)")

print("\n" + "=" * 60)
